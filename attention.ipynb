{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention公式\n",
    "\n",
    "$$ Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "1. matmul和 @作用一样\n",
    "2. 除以$\\sqrt{d_k}$是因为1）防止梯度消失(由于softmax在非常大和非常小的时候都梯度都接近于0)？让QK内积的分布保持和输入一致。\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2493,  0.3050,  0.2408, -0.3359],\n",
       "         [-0.2715,  0.3370,  0.2590, -0.2732]],\n",
       "\n",
       "        [[-0.2894,  0.3821,  0.3016, -0.2133],\n",
       "         [-0.2953,  0.3865,  0.3124, -0.2028]],\n",
       "\n",
       "        [[-0.3192,  0.4417,  0.1703, -0.3778],\n",
       "         [-0.3195,  0.4415,  0.1687, -0.3757]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 简化版本\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class selfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        ## 初始化 Q K V 三个线性映射层\n",
    "        self.query = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X shape (batch_size, seq_len,hidden_dim)\n",
    "        # Q K V初始化\n",
    "        Q = self.query(X)\n",
    "        K = self.key(X)\n",
    "        V = self.value(X)\n",
    "        # QKV 为 batchsize,seq_len hidden_dim\n",
    "        \n",
    "        # attention_value： (batch_size, seq_len,seq_len)\n",
    "        ## 分子 ()\n",
    "        # K为 (batch_size ,hidden_dim,seq_len)\n",
    "        attention_value = torch.matmul(\n",
    "            Q,K.transpose(-1,-2)#premute\n",
    "        )# 这里是只变换后两个维度\n",
    "        \n",
    "        #这里得到dim\n",
    "        # (batch_size,seq_len,seq_len)\n",
    "        attention_weight = torch.softmax(attention_value / math.sqrt(self.hidden_dim),dim=-1)\n",
    "        # 开根号的原因：防止乘积后值太大了，可能梯度爆炸。\n",
    "\n",
    "        # print(attention_weight)\n",
    "        # softmax(需要针对哪一个维度？）\n",
    "\n",
    "        # (batch_size,seq,hidden)\n",
    "        output = torch.matmul(attention_weight,V)\n",
    "        return output\n",
    "\n",
    "## test code\n",
    "X = torch.rand(3,2,4)\n",
    "#print(X)\n",
    "\n",
    "net = selfAttention(4)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "mm(input, mat2, *, out=None) -> Tensor\n",
      "\n",
      "Performs a matrix multiplication of the matrices :attr:`input` and :attr:`mat2`.\n",
      "\n",
      "If :attr:`input` is a :math:`(n \\times m)` tensor, :attr:`mat2` is a\n",
      ":math:`(m \\times p)` tensor, :attr:`out` will be a :math:`(n \\times p)` tensor.\n",
      "\n",
      ".. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.\n",
      "          For broadcasting matrix products, see :func:`torch.matmul`.\n",
      "\n",
      "Supports strided and sparse 2-D tensors as inputs, autograd with\n",
      "respect to strided inputs.\n",
      "\n",
      "This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the first matrix to be matrix multiplied\n",
      "    mat2 (Tensor): the second matrix to be matrix multiplied\n",
      "\n",
      "Keyword args:\n",
      "    out (Tensor, optional): the output tensor.\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> mat1 = torch.randn(2, 3)\n",
      "    >>> mat2 = torch.randn(3, 3)\n",
      "    >>> torch.mm(mat1, mat2)\n",
      "    tensor([[ 0.4851,  0.5037, -0.3633],\n",
      "            [-0.0760, -3.6705,  2.4784]])\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.mm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0min_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mout_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m        in_features: size of each input sample\u001b[0m\n",
      "\u001b[0;34m        out_features: size of each output sample\u001b[0m\n",
      "\u001b[0;34m        bias: If set to ``False``, the layer will not learn an additive bias.\u001b[0m\n",
      "\u001b[0;34m            Default: ``True``\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Shape:\u001b[0m\n",
      "\u001b[0;34m        - Input: :math:`(*, H_{in})` where :math:`*` means any number of\u001b[0m\n",
      "\u001b[0;34m          dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\u001b[0m\n",
      "\u001b[0;34m        - Output: :math:`(*, H_{out})` where all but the last dimension\u001b[0m\n",
      "\u001b[0;34m          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Attributes:\u001b[0m\n",
      "\u001b[0;34m        weight: the learnable weights of the module of shape\u001b[0m\n",
      "\u001b[0;34m            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\u001b[0m\n",
      "\u001b[0;34m            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\u001b[0m\n",
      "\u001b[0;34m            :math:`k = \\frac{1}{\\text{in\\_features}}`\u001b[0m\n",
      "\u001b[0;34m        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\u001b[0m\n",
      "\u001b[0;34m                If :attr:`bias` is ``True``, the values are initialized from\u001b[0m\n",
      "\u001b[0;34m                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\u001b[0m\n",
      "\u001b[0;34m                :math:`k = \\frac{1}{\\text{in\\_features}}`\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Examples::\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        >>> m = nn.Linear(20, 30)\u001b[0m\n",
      "\u001b[0;34m        >>> input = torch.randn(128, 20)\u001b[0m\n",
      "\u001b[0;34m        >>> output = m(input)\u001b[0m\n",
      "\u001b[0;34m        >>> print(output.size())\u001b[0m\n",
      "\u001b[0;34m        torch.Size([128, 30])\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0m__constants__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'in_features'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'out_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0min_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mout_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfactory_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# https://github.com/pytorch/pytorch/issues/57109\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfan_in\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfan_in\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m'in_features={}, out_features={}, bias={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m           /opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/linear.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     NonDynamicallyQuantizableLinear, LazyLinear, Linear, LinearBn1d, Linear"
     ]
    }
   ],
   "source": [
    "nn.Linear??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3031, 0.3631, 0.3338],\n",
      "         [0.2562, 0.3624, 0.3814],\n",
      "         [0.2310, 0.3939, 0.3751]],\n",
      "\n",
      "        [[0.2875, 0.3755, 0.3370],\n",
      "         [0.0538, 0.8179, 0.1282],\n",
      "         [0.2403, 0.4491, 0.3106]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4611,  0.3685,  0.0833, -0.3642],\n",
       "         [-0.4932,  0.3379,  0.1168, -0.3600],\n",
       "         [-0.5041,  0.3190,  0.1316, -0.3631]],\n",
       "\n",
       "        [[-0.9265,  0.0881,  0.2654, -0.7706],\n",
       "         [-1.8330, -0.5163,  0.6153, -1.0257],\n",
       "         [-1.0865, -0.0123,  0.3202, -0.8213]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Attention V2 优化\n",
    "# 如果网络比较小，可以将QKV在一起写，效率优化。\n",
    "\n",
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.proj = nn.Linear(hidden_dim,hidden_dim*3)\n",
    "\n",
    "    def forward(self,X):\n",
    "        #X: batchsize,seq,hidden_dim\n",
    "\n",
    "        QKV = self.proj(X) # get (batch_size,seq,hidden_dim*3)\n",
    "        Q,K,V = torch.split(QKV,self.hidden_dim,dim=-1)#split为三个，每个都是hidden_dim大小\n",
    "        attention_weight = torch.softmax(Q@ K.transpose(-1,-2)/math.sqrt(self.hidden_dim),dim=-1)\n",
    "        print(attention_weight)\n",
    "        output = attention_weight@V\n",
    "        return output\n",
    "\n",
    "X = torch.randn(2,3,4)\n",
    "net2 = SelfAttentionV2(4)\n",
    "net2(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在此基础上继续加入细节\n",
    "\n",
    "Self-Attention 还有更多的细节\n",
    "* 有时有dropout；\n",
    "* 一般会加入attention_mask操作，因为样本会padding\n",
    "* MultiHeadAttention过程中，除了QKV三个矩阵之外，还有output投影矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4129, 0.2558, 0.3313, 0.0000],\n",
      "         [0.4294, 0.2701, 0.3005, 0.0000],\n",
      "         [0.3445, 0.3168, 0.3387, 0.0000],\n",
      "         [0.3925, 0.2593, 0.3482, 0.0000]],\n",
      "\n",
      "        [[0.5243, 0.4757, 0.0000, 0.0000],\n",
      "         [0.5098, 0.4902, 0.0000, 0.0000],\n",
      "         [0.4758, 0.5242, 0.0000, 0.0000],\n",
      "         [0.4935, 0.5065, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2368,  0.1474],\n",
       "         [ 0.2398,  0.1432],\n",
       "         [ 0.2582,  0.1145],\n",
       "         [ 0.3062,  0.0471]],\n",
       "\n",
       "        [[-0.0366,  0.5418],\n",
       "         [-0.0368,  0.5421],\n",
       "         [-0.0371,  0.5426],\n",
       "         [-0.0369,  0.5423]],\n",
       "\n",
       "        [[ 0.0981,  0.3499],\n",
       "         [ 0.0981,  0.3499],\n",
       "         [ 0.0981,  0.3499],\n",
       "         [ 0.0981,  0.3499]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropout \n",
    "# attention_mask\n",
    "# outout的映射\n",
    "class SelfAttentionV3(nn.Module):\n",
    "    def __init__(self,hidden_dim,dropOutRate=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.proj = nn.Linear(hidden_dim,hidden_dim*3)\n",
    "        # dropout的位置在哪里？\n",
    "        # attention mask 对一些padding的词汇mask\n",
    "        # output_proj的映射。可选。\n",
    "        self.attentionDropout = nn.Dropout(dropOutRate)\n",
    "        # 每个句子长度不一样，所有计算的时候具有mask\n",
    "\n",
    "        self.output_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "    def forward(self,X,mask=None):\n",
    "        # X()\n",
    "        QKV = self.proj(X)\n",
    "        Q,K,V = torch.split(QKV,self.hidden_dim,dim=-1)\n",
    "        \n",
    "        # (batch_size,seq,seq)\n",
    "        attention_weight = Q@K.transpose(-1,-2)/math.sqrt(self.hidden_dim)\n",
    "        if mask is not None:\n",
    "            # 一种方法就是给其十分小的值。 softmax之后几乎为0\n",
    "            # 给被mask词语十分小的值。\n",
    "            # masked_fill 如果为0时，填充什么。\n",
    "            attention_weight = attention_weight.masked_fill(mask==0,-1e9)\n",
    "        #print(attention_weight)# 查看mask的结果\n",
    "\n",
    "        attention_weight = torch.softmax(attention_weight,dim=-1)\n",
    "        print(attention_weight)# 查看mask的softmax之后的结果的结果\n",
    "\n",
    "\n",
    "        # BRET 这里这么用\n",
    "        # 先dropout之后再乘以V\n",
    "        attention_weight = self.attentionDropout(attention_weight)\n",
    "\n",
    "        attention_result = attention_weight@ V\n",
    "        output = self.output_proj(attention_result)\n",
    "        return output\n",
    "    \n",
    "X = torch.randn(3,4,2)\n",
    "b = torch.tensor(\n",
    "    [\n",
    "        [1,1,1,0], #第一个句子只padding为3\n",
    "        [1,1,0,0], # 第二个padding两个元素\n",
    "        [1,0,0,0],  # 第三个padding3个元素\n",
    "    ]\n",
    ")\n",
    "#b.shape\n",
    "mask = b.unsqueeze(dim=1).repeat(1,4,1) # unsqueeze扩充一个维度，在1\n",
    "\n",
    "\n",
    "net = SelfAttentionV3(2)\n",
    "net(X,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_DropoutNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"During training, randomly zeroes some of the elements of the input\u001b[0m\n",
      "\u001b[0;34m    tensor with probability :attr:`p` using samples from a Bernoulli\u001b[0m\n",
      "\u001b[0;34m    distribution. Each channel will be zeroed out independently on every forward\u001b[0m\n",
      "\u001b[0;34m    call.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This has proven to be an effective technique for regularization and\u001b[0m\n",
      "\u001b[0;34m    preventing the co-adaptation of neurons as described in the paper\u001b[0m\n",
      "\u001b[0;34m    `Improving neural networks by preventing co-adaptation of feature\u001b[0m\n",
      "\u001b[0;34m    detectors`_ .\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Furthermore, the outputs are scaled by a factor of :math:`\\frac{1}{1-p}` during\u001b[0m\n",
      "\u001b[0;34m    training. This means that during evaluation the module simply computes an\u001b[0m\n",
      "\u001b[0;34m    identity function.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m        p: probability of an element to be zeroed. Default: 0.5\u001b[0m\n",
      "\u001b[0;34m        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Shape:\u001b[0m\n",
      "\u001b[0;34m        - Input: :math:`(*)`. Input can be of any shape\u001b[0m\n",
      "\u001b[0;34m        - Output: :math:`(*)`. Output is of the same shape as input\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Examples::\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        >>> m = nn.Dropout(p=0.2)\u001b[0m\n",
      "\u001b[0;34m        >>> input = torch.randn(20, 16)\u001b[0m\n",
      "\u001b[0;34m        >>> output = m(input)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    .. _Improving neural networks by preventing co-adaptation of feature\u001b[0m\n",
      "\u001b[0;34m        detectors: https://arxiv.org/abs/1207.0580\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m           /opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/dropout.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     Dropout"
     ]
    }
   ],
   "source": [
    "nn.Dropout??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "masked_fill_(mask, value)\n",
      "\n",
      "Fills elements of :attr:`self` tensor with :attr:`value` where :attr:`mask` is\n",
      "True. The shape of :attr:`mask` must be\n",
      ":ref:`broadcastable <broadcasting-semantics>` with the shape of the underlying\n",
      "tensor.\n",
      "\n",
      "Args:\n",
      "    mask (BoolTensor): the boolean mask\n",
      "    value (float): the value to fill in with\n",
      "\u001b[0;31mType:\u001b[0m      method_descriptor"
     ]
    }
   ],
   "source": [
    "torch.Tensor.masked_fill_??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3652, 1.0789],\n",
       "         [0.3655, 1.0783],\n",
       "         [0.3647, 1.0796],\n",
       "         [0.3651, 1.0790]],\n",
       "\n",
       "        [[0.4184, 0.9227],\n",
       "         [0.4183, 0.9227],\n",
       "         [0.0000, 0.0000],\n",
       "         [0.4184, 0.9227]],\n",
       "\n",
       "        [[0.3013, 1.2024],\n",
       "         [0.3013, 1.2024],\n",
       "         [0.3013, 1.2024],\n",
       "         [0.3013, 1.2024]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 第四种self-attention\n",
    "class SelfAttentionV4(nn.Module):\n",
    "    def __init__(self,dim,dropOut_rate= 0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        self.query = nn.Linear(dim,dim) # 不写bias为False\n",
    "        self.key = nn.Linear(dim,dim)\n",
    "        self.vaule = nn.Linear(dim,dim)\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(dropOut_rate)\n",
    "\n",
    "    def forward(self,X,mask=None):\n",
    "        # X : (batch_size ,seq, dim)\n",
    "        Q = self.query(X)\n",
    "        K = self.key(X)\n",
    "        V = self.vaule(X)\n",
    "        # batch_size,seq,dim * (batch_size,dim,seq) => batchsize,seq,seq\n",
    "        # attention_weight\n",
    "        attention_weight = Q@K.transpose(-1,-2)/math.sqrt(self.dim)\n",
    "        # mask操作\n",
    "        if mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(mask==0,-1e9) #(float(\"-inf\"))\n",
    "        # softmax操作 \n",
    "        attention_weight = torch.softmax(attention_weight,dim=-1)\n",
    "\n",
    "        # dropout\n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        # (batch_size,seq,seq) * (batchsize,seq,dim) =>\n",
    "        #out \n",
    "        # (btach_size,seq,dim)\n",
    "        output = attention_weight@V\n",
    "        # 这里是得到了饿\n",
    "        return output\n",
    "    \n",
    "X = torch.rand(3,4,2)\n",
    "mask = torch.Tensor([\n",
    "    [1,1,1,0],\n",
    "    [1,1,0,0],\n",
    "    [1,0,0,0]\n",
    "])\n",
    "# 原先是一个3x4的mask，然后在列上添加一个新维度得到\n",
    "# 3x1x4的的tensor\n",
    "# 然后再再\n",
    "mask = mask.unsqueeze(dim=1).repeat(1,4,1)\n",
    "# unsqueeze得到(3,4,4)大小的mask\n",
    "# 在进行mask时，我们的attention_weight的shape是(batch_size,seq,seq)也就是(3,4,4)所以能够进行mask\n",
    "net = SelfAttentionV4(2)\n",
    "net(X,mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "unsqueeze(input, dim) -> Tensor\n",
      "\n",
      "Returns a new tensor with a dimension of size one inserted at the\n",
      "specified position.\n",
      "\n",
      "The returned tensor shares the same underlying data with this tensor.\n",
      "\n",
      "A :attr:`dim` value within the range ``[-input.dim() - 1, input.dim() + 1)``\n",
      "can be used. Negative :attr:`dim` will correspond to :meth:`unsqueeze`\n",
      "applied at :attr:`dim` = ``dim + input.dim() + 1``.\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the input tensor.\n",
      "    dim (int): the index at which to insert the singleton dimension\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> x = torch.tensor([1, 2, 3, 4])\n",
      "    >>> torch.unsqueeze(x, 0)\n",
      "    tensor([[ 1,  2,  3,  4]])\n",
      "    >>> torch.unsqueeze(x, 1)\n",
      "    tensor([[ 1],\n",
      "            [ 2],\n",
      "            [ 3],\n",
      "            [ 4]])\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "torch.unsqueeze??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ ](./images/multi_head_attention.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 最后一步，就是multi-head的书写\n",
    "# 上述得到的都是self-attention单个的实现，实际上要使用multi-head的实现。\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    #先进行QKV然后对每一个得到的Linaer层进行attention然后进行concat再进行Linear\n",
    "    def __init__(self,hidden_dim,heads):\n",
    "        super().__init__()\n",
    "        self.dim = hidden_dim\n",
    "        self.heads = heads\n",
    "\n",
    "        self.query = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "        self.head_dim = hidden_dim//heads\n",
    "        self.attention = SelfAttentionV4(self.head_dim)\n",
    "\n",
    "        #这么记忆：多头注意力是其中的不同部分，所以分块去注意每个head，然后将其concat起来\n",
    "\n",
    "        self.Concat_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "    \n",
    "    def forward(self,X,mask=None):\n",
    "        batch_size,seq_len,_ =X.shape\n",
    "        # X(batch_size,seq_len,hidden_dim)\n",
    "        Q = self.query(X)\n",
    "        K = self.key(X)\n",
    "        V = self.value(X)\n",
    "        # 进行完了第一步 \n",
    "        # shape要变成 (batch_size,num_head,seq,head_dim)\n",
    "        Q = Q.view(batch_size,seq_len,self.heads,self.head_dim).permute(0,-2,1,-1)\n",
    "        K = K.view(batch_size,seq_len,self.heads,self.head_dim).permute(0,-2,1,-1)\n",
    "        V = V.view(batch_size,seq_len,self.heads,self.head_dim).permute(0,-2,1,-1)\n",
    "\n",
    "        # scaled-dot product attention\n",
    "        attention_weight = Q@K.transpose(-1,-2)/math.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(mask==0,float(\"-inf\"))\n",
    "\n",
    "        # softmax \n",
    "        attention_weight = torch.softmax(attention_weight,dim=-1)\n",
    "        single_attention = attention_weight @ V\n",
    "\n",
    "        ## 将数据重新变成\n",
    "        # batch_size,seq_len,num_heads,head_dim\n",
    "        single_attention = single_attention.transpose(1,2).contiguous()\n",
    "\n",
    "        # 然后concat起来，也就是变成\n",
    "        # batch_size,seq,hidden_dim\n",
    "        output = single_attention.view(batch_size,seq_len,-1)\n",
    "        output = self.Concat_proj(output)\n",
    "        return output\n",
    "\n",
    "X = torch.rand(3,2,128)\n",
    "mask = torch.Tensor([\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [0,0]\n",
    "])\n",
    "# 注意这里是多头的mask\n",
    "# 其shape应该是 (batch_size,num_head,seq,seq)\n",
    "# (3,8,2,2)\n",
    "# (3,2)->(3,1,2)->(3,1,1,2)\n",
    "mask = mask.unsqueeze(dim=1).unsqueeze(dim=2).repeat(1,8,2,1)\n",
    "print(mask.shape)\n",
    "\n",
    "net = MultiHeadAttention(128,8)\n",
    "## 这里用8个头 hidden_dim是128\n",
    "## 128/8 = 16\n",
    "net(X,mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiheadAttentionV2(nn.Module):\n",
    "    def __init__(self,hidden_dim,nums_head):\n",
    "        super().__init__()\n",
    "        self.nums_head = nums_head\n",
    "        self.attention = SelfAttentionV4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "view(*shape) -> Tensor\n",
      "\n",
      "Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      "different :attr:`shape`.\n",
      "\n",
      "The returned tensor shares the same data and must have the same number\n",
      "of elements, but may have a different size. For a tensor to be viewed, the new\n",
      "view size must be compatible with its original size and stride, i.e., each new\n",
      "view dimension must either be a subspace of an original dimension, or only span\n",
      "across original dimensions :math:`d, d+1, \\dots, d+k` that satisfy the following\n",
      "contiguity-like condition that :math:`\\forall i = d, \\dots, d+k-1`,\n",
      "\n",
      ".. math::\n",
      "\n",
      "  \\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]\n",
      "\n",
      "Otherwise, it will not be possible to view :attr:`self` tensor as :attr:`shape`\n",
      "without copying it (e.g., via :meth:`contiguous`). When it is unclear whether a\n",
      ":meth:`view` can be performed, it is advisable to use :meth:`reshape`, which\n",
      "returns a view if the shapes are compatible, and copies (equivalent to calling\n",
      ":meth:`contiguous`) otherwise.\n",
      "\n",
      "Args:\n",
      "    shape (torch.Size or int...): the desired size\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> x = torch.randn(4, 4)\n",
      "    >>> x.size()\n",
      "    torch.Size([4, 4])\n",
      "    >>> y = x.view(16)\n",
      "    >>> y.size()\n",
      "    torch.Size([16])\n",
      "    >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
      "    >>> z.size()\n",
      "    torch.Size([2, 8])\n",
      "\n",
      "    >>> a = torch.randn(1, 2, 3, 4)\n",
      "    >>> a.size()\n",
      "    torch.Size([1, 2, 3, 4])\n",
      "    >>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
      "    >>> b.size()\n",
      "    torch.Size([1, 3, 2, 4])\n",
      "    >>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n",
      "    >>> c.size()\n",
      "    torch.Size([1, 3, 2, 4])\n",
      "    >>> torch.equal(b, c)\n",
      "    False\n",
      "\n",
      "\n",
      ".. method:: view(dtype) -> Tensor\n",
      "   :noindex:\n",
      "\n",
      "Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      "different :attr:`dtype`.\n",
      "\n",
      "If the element size of :attr:`dtype` is different than that of ``self.dtype``,\n",
      "then the size of the last dimension of the output will be scaled\n",
      "proportionally.  For instance, if :attr:`dtype` element size is twice that of\n",
      "``self.dtype``, then each pair of elements in the last dimension of\n",
      ":attr:`self` will be combined, and the size of the last dimension of the output\n",
      "will be half that of :attr:`self`. If :attr:`dtype` element size is half that\n",
      "of ``self.dtype``, then each element in the last dimension of :attr:`self` will\n",
      "be split in two, and the size of the last dimension of the output will be\n",
      "double that of :attr:`self`. For this to be possible, the following conditions\n",
      "must be true:\n",
      "\n",
      "    * ``self.dim()`` must be greater than 0.\n",
      "    * ``self.stride(-1)`` must be 1.\n",
      "\n",
      "Additionally, if the element size of :attr:`dtype` is greater than that of\n",
      "``self.dtype``, the following conditions must be true as well:\n",
      "\n",
      "    * ``self.size(-1)`` must be divisible by the ratio between the element\n",
      "      sizes of the dtypes.\n",
      "    * ``self.storage_offset()`` must be divisible by the ratio between the\n",
      "      element sizes of the dtypes.\n",
      "    * The strides of all dimensions, except the last dimension, must be\n",
      "      divisible by the ratio between the element sizes of the dtypes.\n",
      "\n",
      "If any of the above conditions are not met, an error is thrown.\n",
      "\n",
      ".. warning::\n",
      "\n",
      "    This overload is not supported by TorchScript, and using it in a Torchscript\n",
      "    program will cause undefined behavior.\n",
      "\n",
      "\n",
      "Args:\n",
      "    dtype (:class:`torch.dtype`): the desired dtype\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> x = torch.randn(4, 4)\n",
      "    >>> x\n",
      "    tensor([[ 0.9482, -0.0310,  1.4999, -0.5316],\n",
      "            [-0.1520,  0.7472,  0.5617, -0.8649],\n",
      "            [-2.4724, -0.0334, -0.2976, -0.8499],\n",
      "            [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
      "    >>> x.dtype\n",
      "    torch.float32\n",
      "\n",
      "    >>> y = x.view(torch.int32)\n",
      "    >>> y\n",
      "    tensor([[ 1064483442, -1124191867,  1069546515, -1089989247],\n",
      "            [-1105482831,  1061112040,  1057999968, -1084397505],\n",
      "            [-1071760287, -1123489973, -1097310419, -1084649136],\n",
      "            [-1101533110,  1073668768, -1082790149, -1088634448]],\n",
      "        dtype=torch.int32)\n",
      "    >>> y[0, 0] = 1000000000\n",
      "    >>> x\n",
      "    tensor([[ 0.0047, -0.0310,  1.4999, -0.5316],\n",
      "            [-0.1520,  0.7472,  0.5617, -0.8649],\n",
      "            [-2.4724, -0.0334, -0.2976, -0.8499],\n",
      "            [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
      "\n",
      "    >>> x.view(torch.cfloat)\n",
      "    tensor([[ 0.0047-0.0310j,  1.4999-0.5316j],\n",
      "            [-0.1520+0.7472j,  0.5617-0.8649j],\n",
      "            [-2.4724-0.0334j, -0.2976-0.8499j],\n",
      "            [-0.2109+1.9913j, -0.9607-0.6123j]])\n",
      "    >>> x.view(torch.cfloat).size()\n",
      "    torch.Size([4, 2])\n",
      "\n",
      "    >>> x.view(torch.uint8)\n",
      "    tensor([[  0, 202, 154,  59, 182, 243, 253, 188, 185, 252, 191,  63, 240,  22,\n",
      "               8, 191],\n",
      "            [227, 165,  27, 190, 128,  72,  63,  63, 146, 203,  15,  63,  22, 106,\n",
      "              93, 191],\n",
      "            [205,  59,  30, 192, 112, 206,   8, 189,   7,  95, 152, 190,  12, 147,\n",
      "              89, 191],\n",
      "            [ 43, 246,  87, 190, 235, 226, 254,  63, 111, 240, 117, 191, 177, 191,\n",
      "              28, 191]], dtype=torch.uint8)\n",
      "    >>> x.view(torch.uint8).size()\n",
      "    torch.Size([4, 16])\n",
      "\u001b[0;31mType:\u001b[0m      method_descriptor"
     ]
    }
   ],
   "source": [
    "torch.Tensor.view??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "contiguous(memory_format=torch.contiguous_format) -> Tensor\n",
      "\n",
      "Returns a contiguous in memory tensor containing the same data as :attr:`self` tensor. If\n",
      ":attr:`self` tensor is already in the specified memory format, this function returns the\n",
      ":attr:`self` tensor.\n",
      "\n",
      "Args:\n",
      "    memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      "        returned Tensor. Default: ``torch.contiguous_format``.\n",
      "\u001b[0;31mType:\u001b[0m      method_descriptor"
     ]
    }
   ],
   "source": [
    "torch.Tensor.contiguous??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
